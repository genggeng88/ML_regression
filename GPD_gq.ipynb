{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR with Derivatives\n",
    "\n",
    "A Gaussian process regression is used for mapping from input variables to output variables.\n",
    "\n",
    "In some cases, it can be useful to model not just the output variables, but also their derivatives. \n",
    "\n",
    "Gaussian process with derivatives (GPD) is a variant of GRP which models both output variables and their derivatives.\n",
    "\n",
    "GPD properties:\n",
    "\n",
    "If $f \\sim GP(\\mu, k)$, then\n",
    "$$\n",
    "\\nabla f \\sim GP(\\frac{\\partial}{\\partial x} \\mu, \\frac{\\partial^2}{\\partial x \\partial y}{k})\n",
    "$$\n",
    "\n",
    "In words, if our original model of $f$ is drawn from a GP with a certain mean and kernel function, then the gradient of our model $f$ is drawn from a GP with a related mean and kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take an Example of 1D input\n",
    "\n",
    "Suppose\n",
    "$$\n",
    "\\mu(x) = 0\n",
    "$$\n",
    "and\n",
    "$$\n",
    "k(x, y) = \\exp(-.5 (x-y)^2/l^2)\n",
    "$$\n",
    "where the number of dimensions is $1$ for simplicity.\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\nabla f \\sim GP(\\frac{d}{dx} \\mu, \\frac{d^2}{d x d y}{k})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "1.\n",
    "$$\n",
    "\\frac{d}{d x} \\mu = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "2.\n",
    "$$\n",
    "\\frac{d}{d x} k(x, y) = -\\frac{x - y}{l^2}k(x, y)\n",
    "$$\n",
    "3. \n",
    "$$\n",
    "\\frac{d^2}{dx dy} k(x, y) = \\frac{1}{l^2}(1 - (x - y)^2/l^2)k(x, y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME for y_error is: 0.05160168445709523\n",
      "RSME for dy_error is: 2.0927974227329497e-08\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaussianRegressionWithDerivatives():\n",
    "    def __init__(self, kernel=None, dd_kernel=None, sigma=1e-6):\n",
    "        self.kernel = kernel\n",
    "        self.dd_kernel = dd_kernel\n",
    "        self.sigma = sigma\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "    def fit(self, X, y, dy):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dy = dy\n",
    "        # print('the shape of self.kernel(X, X) is: ' + str(self.kernel(X, X).shape))\n",
    "        self.K = self.kernel(X, X) + self.sigma**2 * cp.eye(len(X))\n",
    "        # print('the shape of self.K is: ' + str(self.K.shape))\n",
    "        self.dK = self.dd_kernel(X, X) + self.sigma**2 * cp.eye(len(X))\n",
    "        self.L = cp.linalg.cholesky(self.K)\n",
    "        self.dL = cp.linalg.cholesky(self.dK)\n",
    "        # let alpha = inv(K) * y => K * alpha = y => L*L.T * alpha = y => alpha = solve(L.T, solve(L, y))\n",
    "        self.alpha = cp.linalg.solve(self.L.T, cp.linalg.solve(self.L, y))\n",
    "        self.dalpha = cp.linalg.solve(self.dL.T, cp.linalg.solve(self.dL, dy))\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def predict(self, Xtest):\n",
    "        K_train_test = self.kernel(self.X, Xtest)\n",
    "        y_pred = K_train_test.T @ self.alpha\n",
    "\n",
    "        dK_train_test = self.dd_kernel(self.X, Xtest)\n",
    "        dy_pred = dK_train_test.T @ self.dalpha\n",
    "\n",
    "        # L*v = K => v = inv(L)*K => v.T * v = K.T * inv(L.T) * inv(L) * K = K.T * inv(L*L.T) * K\n",
    "        # v = cp.linalg.solve(self.L, K_train_test)\n",
    "        # cov = self.kernel(Xtest, Xtest) - v.T @ v\n",
    "        # std = cp.sqrt(cp.diag(cov))\n",
    "        # return mean, cov, std\n",
    "        return y_pred, dy_pred\n",
    "\n",
    "def d_mu(x):\n",
    "    return 0\n",
    "\n",
    "def RBF(x1, x2, length_scale=1.0):\n",
    "    diff_mat = x1[:, cp.newaxis] - x2\n",
    "    return cp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2)\n",
    "\n",
    "def d_RBF(x1, x2, length_scale=1.0):\n",
    "    diff_mat = x1[:, cp.newaxis] - x2\n",
    "    return -diff_mat * cp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2) / length_scale ** 2\n",
    "\n",
    "def dd_RBF(x1, x2, length_scale=1.0):\n",
    "    diff_mat = x1[:, cp.newaxis] - x2\n",
    "    return (1 - diff_mat**2/length_scale**2)/length_scale**2 * cp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2)\n",
    "\n",
    "def RMSE(y, y_pred):\n",
    "    return cp.sqrt(cp.mean((y_pred - y) ** 2))\n",
    "    \n",
    "X = cp.array(cp.random.uniform(-5, 5, size=(50,)))\n",
    "y = cp.array(cp.sin(X) + cp.random.normal(0, 0.1, size=(50,)))\n",
    "dy = cp.array(cp.cos(X))\n",
    "\n",
    "gr = GaussianRegressionWithDerivatives(RBF, dd_RBF)\n",
    "gr.fit(X,y,dy)\n",
    "\n",
    "# use the same data set as training, y_pred and dy_pred should be similar to y and dy\n",
    "y_pred, dy_pred = gr.predict(X)\n",
    "\n",
    "y_error = RMSE(cp.array(y), cp.array(y_pred))\n",
    "dy_error = RMSE(cp.array(dy), cp.array(dy_pred))\n",
    "\n",
    "'''\n",
    "print('y is: \\n' + str(y[:10]))\n",
    "print('y_pred is: \\n' + str(y_pred[:10]))\n",
    "print('dy is: \\n' + str(dy[:10]))\n",
    "print('dy_pred: \\n' + str(dy_pred[:10]))\n",
    "'''\n",
    "\n",
    "print('RSME for y_error is: ' + str(y_error))\n",
    "print('RSME for dy_error is: ' + str(dy_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above example missed the cross kernel parts, and it works only when we feed derivatives to the model\n",
    "Modity it to get a more general GPD model:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "f \\\\\n",
    "\\nabla f\n",
    "\\end{pmatrix} \\sim GP\\left(\n",
    "\\begin{pmatrix}\n",
    "\\mu(x) \\\\\n",
    "\\frac{d}{dx} \\mu(x)\n",
    "\\end{pmatrix}\n",
    ", \n",
    "\\begin{pmatrix}\n",
    "k(x, y) & \\frac{d}{dy}k(x, y)^T \\\\\n",
    "\\frac{d}{dx}k(x, y) & \\frac{d^2}{d x d y}{k}(x, y) \\\\\n",
    "\\end{pmatrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use jax for following implementations since jax can return gradients (numpy and cupy don't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_dx from d_RBF(): \n",
      "[[ 0.         -0.60653067 -0.27067056]\n",
      " [ 0.60653067  0.         -0.60653067]\n",
      " [ 0.27067056  0.60653067  0.        ]]\n",
      "kernel_dx from jax.jacrev(): \n",
      "[[ 0.         -0.60653067 -0.27067056]\n",
      " [ 0.60653067  0.         -0.60653067]\n",
      " [ 0.27067056  0.60653067  0.        ]]\n",
      "kernel_dx_dy from d_RBF(): \n",
      "[[ 1.          0.         -0.40600586]\n",
      " [ 0.          1.          0.        ]\n",
      " [-0.40600586  0.          1.        ]]\n",
      "kernel_dx_dy from jax.jacrev(): \n",
      "[[ 1.          0.         -0.40600586]\n",
      " [ 0.          1.          0.        ]\n",
      " [-0.40600586  0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Validating my d_RBF() and dd_RBF\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad\n",
    "from jax.scipy.linalg import solve_triangular\n",
    "from functools import partial\n",
    "\n",
    "def RBF(x1, x2, length_scale=1.0):\n",
    "    # The reason we write it this way is because the second derivative\n",
    "    # when x=y does not seem to work ...\n",
    "    diff_mat = x1[:, jnp.newaxis] - x2\n",
    "    return jnp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2)\n",
    "\n",
    "def d_RBF(x1, x2, length_scale=1.0):\n",
    "    diff_mat = x1[:, jnp.newaxis] - x2\n",
    "    return -diff_mat * jnp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2) / length_scale ** 2\n",
    "\n",
    "def dd_RBF(x1, x2, length_scale=1.0):\n",
    "    diff_mat = x1[:, jnp.newaxis] - x2\n",
    "    return (1 - diff_mat**2/length_scale**2)/length_scale**2 * jnp.exp(-0.5 * diff_mat ** 2 / length_scale ** 2)\n",
    "\n",
    "def RMSE(y, y_pred):\n",
    "    return jnp.sqrt(jnp.mean((y_pred - y) ** 2))\n",
    "\n",
    "# use jax.jexrev() and jax.jacfwd() to validate my own d_RBF() and dd_RBF()\n",
    "jax_dx = jax.jacrev(RBF, argnums=0)\n",
    "jax_dx_dy = jax.jacfwd(jax.jacrev(RBF, argnums=0), argnums=1)\n",
    "\n",
    "\n",
    "x1 = jnp.array([1.,2.,3.])\n",
    "x2 = jnp.array([1.,2.,3.])\n",
    "\n",
    "kernel_dx = -d_RBF(x1, x2)\n",
    "jax_kernel_dx = np.sum(jax_dx(x1, x2), axis=0)\n",
    "\n",
    "kernel_dx_dy = dd_RBF(x1, x2)\n",
    "jax_kernel_dx_dy = np.sum(np.sum(jax_dx_dy(x1, x2), axis=0), axis=0)\n",
    "\n",
    "print(\"kernel_dx from d_RBF(): \\n\" + str(kernel_dx))\n",
    "print(\"kernel_dx from jax.jacrev(): \\n\" + str(jax_kernel_dx))\n",
    "\n",
    "print(\"kernel_dx_dy from d_RBF(): \\n\" + str(kernel_dx_dy))\n",
    "print(\"kernel_dx_dy from jax.jacrev(): \\n\" + str(jax_kernel_dx_dy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.60653067  0.13533528  0.         -0.60653067 -0.27067056]\n",
      " [ 0.60653067  1.          0.60653067  0.60653067  0.         -0.60653067]\n",
      " [ 0.13533528  0.60653067  1.          0.27067056  0.60653067  0.        ]\n",
      " [-0.          0.60653067  0.27067056  1.          0.         -0.40600586]\n",
      " [-0.60653067 -0.          0.60653067  0.          1.          0.        ]\n",
      " [-0.27067056 -0.60653067 -0.         -0.40600586  0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def kernal_derivative(x1, x2):\n",
    "    kern = RBF(x1, x2)\n",
    "    left = d_RBF(x1, x2)\n",
    "    right = -d_RBF(x1, x2)\n",
    "    hessian = dd_RBF(x1, x2)\n",
    "\n",
    "    # Put everything together\n",
    "    top = jnp.concatenate([kern, right], axis=1)\n",
    "    bot = jnp.concatenate([left, hessian], axis=1)\n",
    "    K = jnp.concatenate([top, bot])\n",
    "    return K\n",
    "\n",
    "K = kernal_derivative(x1,x2)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a GP to both data and derivatives:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "f \\\\\n",
    "\\nabla f\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "\\mu(x) \\\\\n",
    "\\frac{d}{dx} \\mu(x)\n",
    "\\end{pmatrix} + \n",
    "\n",
    "\\begin{pmatrix}\n",
    "k*(x, y) & \\frac{d}{dy}k*(x, y)^T \\\\\n",
    "\\frac{d}{dx}k*(x, y) & \\frac{d^2}{d x d y}{k*}(x, y) \\\\\n",
    "\\end{pmatrix} * \n",
    "\n",
    "\\begin{pmatrix}\n",
    "k(x, y) & \\frac{d}{dy}k(x, y)^T \\\\\n",
    "\\frac{d}{dx}k(x, y) & \\frac{d^2}{d x d y}{k}(x, y) \\\\\n",
    "\\end{pmatrix} ^{-1} * \n",
    "\n",
    "\\begin{pmatrix}\n",
    "f - mu(x)\\\\\n",
    "f - dmu(x)\n",
    "\\end{pmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "Fit a GP to derivatives:\n",
    " \n",
    "$$ \\hat{y} = \\frac{\\partial}{\\partial x}\\mu(X_*) + K_{*X} (K_{XX} - \\sigma^2 I)^{-1} (y_{X} - \\frac{\\partial}{\\partial x}\\mu(X)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of res_total is: (4,)\n",
      "[18.60653  10.713235] [  2.9019544 -13.275953 ]\n"
     ]
    }
   ],
   "source": [
    "class gprDerivatives():\n",
    "    def __init__(self, kernel=None, sigma=1e-6):\n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = np.concatenate([y, y])\n",
    "        self.K = kernal_derivative(X, X)\n",
    "        self.L = np.linalg.cholesky(self.K)\n",
    "        self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.y))\n",
    "        '''\n",
    "        print('the shape of self.K is: ' + str(self.K.shape))\n",
    "        print('the shape of self.y is: ' + str(self.y.shape))\n",
    "        print('the shape of self.L is: ' + str(self.L.shape))\n",
    "        print('the shape of self.alpha is: ' + str(self.alpha.shape))\n",
    "        '''\n",
    "        # let alpha = inv(K) * y => K * alpha = y => L*L.T * alpha = y => alpha = solve(L.T, solve(L, y))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, Xtest):\n",
    "        K_train_test = kernal_derivative(X, Xtest)\n",
    "        n = Xtest.shape[0]\n",
    "        res_total = K_train_test.T @ self.alpha\n",
    "        y_pred = res_total[:n]\n",
    "        dy_pred =  res_total[n:]\n",
    "\n",
    "        # L*v = K => v = inv(L)*K => v.T * v = K.T * inv(L.T) * inv(L) * K = K.T * inv(L*L.T) * K\n",
    "        # v = cp.linalg.solve(self.L, K_train_test)\n",
    "        # cov = self.kernel(Xtest, Xtest) - v.T @ v\n",
    "        # std = cp.sqrt(cp.diag(cov))\n",
    "        # return mean, cov, std\n",
    "        return y_pred, dy_pred\n",
    "\n",
    "X = np.array([1.,2.,3.])\n",
    "y = np.array([1.,4.,9.])\n",
    "Xtest = np.array([4.,5.])\n",
    "\n",
    "gpd = gprDerivatives(kernel=RBF)\n",
    "gpd.fit(X,y)\n",
    "y_pred, dy_pred = gpd.predict(Xtest)\n",
    "\n",
    "print(y_pred, dy_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
