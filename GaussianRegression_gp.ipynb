{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bed7e5",
   "metadata": {},
   "source": [
    "## Guassian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution vs. Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Distribution (Normal Distribution) describes the shape or probability of how data points in a dataset will be distributed, with the expression of: \n",
    "\n",
    "$$X \\sim N(\\mu, \\Sigma)$$\n",
    "\n",
    "Gaussian process a probability distribution over possible functions instead of data points. The idea is to model the unknown function that relates the input and output variables as a random draw from a Gaussian process. This allows us to make predictions on new input values by calculating the conditional distribution of the output variable given the input values and the training data. GP is fully expressed by its mean function and kernel function: \n",
    "\n",
    "$$X \\sim GP(m(x),  k(x,x'))$$\n",
    "\n",
    "$m(x)$: the mean function, describes the mean of any given data point x\n",
    "$k(x,x')$: the kernel function, describes the relationship between any given two data points x and x'.\n",
    "\n",
    "Kernel function: defines the covariance between different points in the input space, which determines how closely related these points are in terms of their output values. \n",
    "\n",
    "The most common kernel function is the radial basis function (RBF) kernel: \n",
    "\n",
    "$$k(x1, x2) = exp-\\dfrac{(x1 - x2)^2}{2L^2}$$\n",
    "\n",
    "One advantage of Gaussian regression is that it provides a probabilistic estimate of the predicted output, along with a measure of uncertainty. This is in contrast to other regression techniques, such as linear regression or decision trees, which only provide a point estimate of the predicted output. Additionally, Gaussian regression can be used for tasks such as Bayesian optimization, where the goal is to find the input values that maximize a given objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/922361294/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:411: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\"The optimal value found for \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPR example using scikit-learn packages:\n",
    "# reference: https://towardsdatascience.com/getting-started-with-gaussian-process-regression-modeling-47e7982b534d \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, WhiteKernel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# use a combined kernel\n",
    "kernel = ConstantKernel(1.0) + ConstantKernel(1.0) * RBF(10) + WhiteKernel(5)\n",
    "model = GaussianProcessRegressor(kernel=kernel)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_tr, y_pred_tr_std = model.predict(X_train, return_std=True)\n",
    "y_pred_te, y_pred_te_std = model.predict(X_test, return_std=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(y_test, y_pred_te, yerr=y_pred_te_std, fmt='o')\n",
    "plt.title('Gaussian process regression, R2=%.2f' % r2_score(y_test, y_pred_te))\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a GP to data:\n",
    " \n",
    "$$ \\hat{y} = \\mu(X_*) + K_{*X} (K_{XX} + \\sigma^2 I)^{-1} (y_{X} - \\mu(X)) $$\n",
    "\n",
    "$K_{*X}$ is the covariance matrix between test and training data, $K_{XX}$ is the covariance matrix between training and training data, $y_X$ is the predictive output from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70290731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40000034 0.59999354 9.40000547]\n",
      "[1.4 0.6 9.4]\n"
     ]
    }
   ],
   "source": [
    "# write my own kernel and GP model\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Radial basis function (RBF) kernel\n",
    "def RBF(x1, x2, length_scale=1.0):\n",
    "    x1=cp.array(x1)\n",
    "    x2=cp.array(x2)\n",
    "    return cp.exp(-0.5 * (x1 - x2)**2 / length_scale ** 2)\n",
    "'''\n",
    "x1 = [[1,1,1],[2,2,2],[3,3,3]]\n",
    "x2 = [[4,5,6], [4,5,6], [4,5,6]]\n",
    "y = RBF(x1, x2)\n",
    "print(y)\n",
    "'''\n",
    "\n",
    "# kernel/covariance matrix\n",
    "def kernel_matrix(kernel, X1, X2, L=1.0, sigma=1e-6):\n",
    "    # let's first assum X1 and X2 are both one-dimensional, can use cp.hstack() to transform them into one-dimension\n",
    "    size1 = X1.shape[0]\n",
    "    size2 = X2.shape[0]\n",
    "    X1 = cp.reshape(X1, [size1,1])\n",
    "    X2 = cp.reshape(X2, [1, size2])\n",
    "    m = X1.shape[0]\n",
    "    n = X2.shape[1]\n",
    "    X1_trans = cp.repeat(X1, n, axis=1)\n",
    "    X2_trans = cp.repeat(X2, m, axis=0) \n",
    "    #print(X1_trans)\n",
    "    #print(X2_trans)\n",
    "    return kernel(X1,X2,L)\n",
    "\n",
    "def mu(x):\n",
    "    return x\n",
    "\n",
    "def GP_fit(x_train, x_test, y_train, y_test, k, mu, kernel_matrix, L = 1.0, sigma = 1e-6):\n",
    "    # GP_fit with this function:\n",
    "    # y = mu(X_*) + K_{*X} (K_{XX} + \\sigma^2 I)^{-1} (y_{X} - \\mu(X))\n",
    "    \n",
    "    K_train_test = kernel_matrix(k, x_test, x_train, L, sigma)\n",
    "    K_train_train = kernel_matrix(k, x_train, x_train, L, sigma)\n",
    "\n",
    "    K_y = cp.linalg.inv(K_train_train - (sigma * cp.identity(x_train.shape[0]) ))\n",
    "    post_mean = mu(x_test) + ( (K_train_test @ K_y) @ (y_train - mu(x_train)) )\n",
    "    return post_mean\n",
    "\n",
    "'''\n",
    "x1 = cp.array([1,2,3,4])\n",
    "x2 = cp.array([5,6])\n",
    "kernel_mat = kernel_matrix(RBF, x1, x2)\n",
    "print(kernel_mat)\n",
    "'''\n",
    "\n",
    "X_train = cp.array([1., 7., 4.])\n",
    "X_test = cp.array([2.5, 5.3])\n",
    "\n",
    "y_train = cp.array([1.4,0.6,9.4])\n",
    "y_test = cp.array([4.,1.])\n",
    "\n",
    "# use the same dataset for both training and testing\n",
    "post_mean = GP_fit(X_train, X_train, y_train, y_train, RBF, mu, kernel_matrix)\n",
    "print(post_mean)\n",
    "print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above GPR model works for 1-dimensional data input.\n",
    "\n",
    "Write a GPR model which works for multiple dimension data input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question to be answered: <b>Do I need to add mean(x_train) and mean(x_test) in GPR?</b>\n",
    "\n",
    "In Gaussian Process Regression, it is not necessary to subtract the mean of train_x or test_x.\n",
    "\n",
    "The prior over functions is assumed to be a zero-mean Gaussian process. This means that the mean of the data is already accounted for by the kernel function. The kernel function specifies the covariance between points in the input space, and it incorporates both the mean and variance of the data.\n",
    "\n",
    "Thus, it is common to center the training data by subtracting the mean of the train_x from both the train_x and test_x. This can help improve numerical stability and make the optimization problem more well-behaved. However, it is not strictly necessary to do so, as the kernel function is designed to handle data with non-zero means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is: [2. 4. 6.]\n",
      "y_pred is: [2. 4. 6.]\n",
      "cov: \n",
      "[[ 1.99999900e-06  2.47891985e-15 -6.14337930e-18]\n",
      " [ 2.47891985e-15  1.99999900e-06  2.47935353e-15]\n",
      " [-6.14464592e-18  2.47891985e-15  1.99999900e-06]]\n",
      "std: [0.00141421 0.00141421 0.00141421]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "class GaussianRegressor:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.sigma = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, X, y, sigma=1e-6):\n",
    "        self.sigma = sigma\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        K = self.kernel(self.X, self.X) + self.sigma * cp.eye(len(self.X))\n",
    "        K_train_test = self.kernel(self.X, Xtest)\n",
    "        K_test_test = self.kernel(Xtest, Xtest) + self.sigma * cp.eye(len(Xtest))\n",
    "        \n",
    "        #mean = K_train_test.T @ cp.linalg.inv(K) @ self.y\n",
    "\n",
    "        # mu(x_train) and mu(x_test) are not strictly required\n",
    "        mean = mu(Xtest) + K_train_test.T @ cp.linalg.inv(K) @ (self.y - mu(self.X))\n",
    "\n",
    "        # returned cov represents the uncertainty associated with the predicted mean at each test point\n",
    "        cov = K_test_test - K_train_test.T @ cp.linalg.inv(K) @ K_train_test\n",
    "        std = cp.sqrt(cp.diag(cov))\n",
    "\n",
    "        return mean, cov, std\n",
    "\n",
    "# Define the RBF kernel\n",
    "# now X1 and X2 are m1-n and m2-n dimensional matrix\n",
    "# X1 and X2 have the same column number n which denotes the number of featrues\n",
    "# X1 and X2 have different first dimension m1 and m2 which denote number of data point\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    cp.atleast_2d(X1)\n",
    "    cp.atleast_2d(X2)\n",
    "    s1 = X1.shape\n",
    "    s2 = X2.shape\n",
    "    \n",
    "    if (s1[1] != s2[1]):\n",
    "        print(\"ERROR! RBF input matices must have the same number of columns!\")\n",
    "        return\n",
    "\n",
    "    dists = cp.sum((X1[:, cp.newaxis] - X2) ** 2, axis=2)\n",
    "    return cp.exp(-dists / (2 * length_scale ** 2))\n",
    "\n",
    "def mu(x):\n",
    "    n = x.shape[1]\n",
    "    return cp.sum(x,axis=1) / n\n",
    "\n",
    "'''\n",
    "X1 = cp.array([[1.,2.,3.],[3.,4.,5.],[5.,6.,7.]])\n",
    "X2 = cp.array([[2.,3.,4.],[7.,8.,9.],[9.,10,11.]])\n",
    "cov = rbf_kernel(X1, X2)\n",
    "print(cov)\n",
    "\n",
    ">> output is:\n",
    "[[2.23130160e-01 3.53262857e-24 2.03109266e-42]\n",
    " [2.23130160e-01 3.77513454e-11 3.53262857e-24]\n",
    " [1.37095909e-06 2.47875218e-03 3.77513454e-11]]\n",
    "'''\n",
    "\n",
    "X1 = cp.array([[1.,2.,3.],[3.,4.,5.],[5.,6.,7.]])\n",
    "y = cp.array([2.,4.,6.])\n",
    "gr = GaussianRegressor(rbf_kernel)\n",
    "gr.fit(X1,y)\n",
    "\n",
    "# use the same data set as training, y_pred should be similar to y\n",
    "y_pred, cov, std = gr.predict(X1)\n",
    "\n",
    "print('y is: ' + str(y))\n",
    "print('y_pred is: ' + str(y_pred))\n",
    "print('cov: \\n' + str(cov))\n",
    "print('std: ' + str(std))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR with Cholesky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is Cholesky?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cholesky decomposition (or factorization) is a more efficiently method for solving a linear system of equations,especially for large and sparse matrices.\n",
    "\n",
    "Every symetric and positive semi-definite matrix $A$ can be decomposed into: $A = L * L^T$\n",
    "\n",
    "where L is a lower triangular matrix. The elements of L are calculated using the following equations:\n",
    "\n",
    "If symetric and positive semi-definite matrix $A$ satisfies the linear system quation $Ax = b$, then this system equation ($L * L^T * x = b$) can be solved more efficiently using Cholesky decomposition with two steps: \n",
    "\n",
    "1. Forward substitution: Solve $Ly = b$ for y using forward substitution.\n",
    "\n",
    "2. Backward substitution: Solve $L^Tx = y$ for x using backward substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is: [2. 4. 6.]\n",
      "y_pred is: [2. 4. 6.]\n",
      "cov: \n",
      "[[1.00008890e-12 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.00019992e-12 4.33680869e-19]\n",
      " [0.00000000e+00 4.33680869e-19 1.00019992e-12]]\n",
      "std: [1.00004445e-06 1.00009996e-06 1.00009996e-06]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "class GaussianRegressorCholesky:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.sigma = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, X, y, sigma=1e-6):\n",
    "        self.sigma = sigma\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.K = self.kernel(X, X) + self.sigma**2 * cp.eye(len(X))\n",
    "        self.L = cp.linalg.cholesky(self.K)\n",
    "        # let alpha = inv(K) * y => K * alpha = y => L*L.T * alpha = y => alpha = solve(L.T, solve(L, y))\n",
    "        self.alpha = cp.linalg.solve(self.L.T, cp.linalg.solve(self.L, y))\n",
    "\n",
    "    def predict(self, Xtest):\n",
    "        K_train_test = self.kernel(self.X, Xtest)\n",
    "        mean = K_train_test.T @ self.alpha\n",
    "\n",
    "        # L*v = K => v = inv(L)*K => v.T * v = K.T * inv(L.T) * inv(L) * K = K.T * inv(L*L.T) * K\n",
    "        v = cp.linalg.solve(self.L, K_train_test)\n",
    "        cov = self.kernel(Xtest, Xtest) - v.T @ v\n",
    "        std = cp.sqrt(cp.diag(cov))\n",
    "\n",
    "        return mean, cov, std\n",
    "\n",
    "\n",
    "X1 = cp.array([[1.,2.,3.],[3.,4.,5.],[5.,6.,7.]])\n",
    "y = cp.array([2.,4.,6.])\n",
    "gr = GaussianRegressorCholesky(rbf_kernel)\n",
    "gr.fit(X1,y)\n",
    "\n",
    "# use the same data set as training, y_pred should be similar to y\n",
    "y_pred, cov, std = gr.predict(X1)\n",
    "\n",
    "print('y is: ' + str(y))\n",
    "print('y_pred is: ' + str(y_pred))\n",
    "print('cov: \\n' + str(cov))\n",
    "print('std: ' + str(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR with Derivatives\n",
    "A Gaussian process is a collection of random variables, any finite subset of which has a joint Gaussian distribution. In machine learning, it is often used as a probabilistic model for regression and classification tasks, where the goal is to learn a mapping from input variables to output variables.\n",
    "\n",
    "In some cases, it can be useful to model not just the output variables, but also their derivatives. This can be done using a variant of the Gaussian process called a Gaussian process with derivatives (GPD). In a GPD, the joint distribution over the function values and their derivatives is still Gaussian, but the covariance function is modified to include information about the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian regression with derivatives model is a type of machine learning model that uses Gaussian functions to model the relationship between a set of input variables and a target variable, as well as the derivatives of the target variable with respect to the input variables. The model is typically used for regression problems, where the goal is to predict a continuous output variable.\n",
    "\n",
    "The basic form of the model is:\n",
    "\n",
    "y(x) = w0 + w1G1(x) + w2G2(x) + ... + wN*GN(x)\n",
    "\n",
    "where y(x) is the target variable, x is a vector of input variables, and G1(x), G2(x), ..., GN(x) are Gaussian functions centered at different points in the input space. The parameters w0, w1, ..., wN are the weights that are learned during training.\n",
    "\n",
    "In addition to modeling the target variable, the model also includes terms for the derivatives of the target variable with respect to the input variables. These terms are given by:\n",
    "\n",
    "dy(x)/dx1 = a1G1'(x) + a2G2'(x) + ... + aNGN'(x)\n",
    "dy(x)/dx2 = b1G1'(x) + b2G2'(x) + ... + bNGN'(x)\n",
    "...\n",
    "dy(x)/dxN = z1G1'(x) + z2G2'(x) + ... + zN*GN'(x)\n",
    "\n",
    "where G1'(x), G2'(x), ..., GN'(x) are the first derivatives of the Gaussian functions, and a1, a2, ..., aN, b1, b2, ..., bN, ..., z1, z2, ..., zN are the weights for the derivative terms.\n",
    "\n",
    "The final form of the model is:\n",
    "\n",
    "y(x) = w0 + w1G1(x) + w2G2(x) + ... + wN*GN(x) +\n",
    "dy(x)/dx1 + dy(x)/dx2 + ... + dy(x)/dxN\n",
    "\n",
    "During training, the weights w0, w1, ..., wN and a1, a2, ..., aN, b1, b2, ..., bN, ..., z1, z2, ..., zN are learned by minimizing the mean squared error between the model predictions and the true values of the target variable. The resulting model can then be used to predict the target variable and its derivatives for new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of X1 after deleting one column is: (3, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2790015/3025356123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianRegressionWithDerivatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# use the same data set as training, y_pred should be similar to y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2790015/3025356123.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the shape of X1 after deleting one column is: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#y1 = self.gpr.fit(X1, cp.gradient(y, axis=0)[:, i]).predict(X1, return_std=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m#dy[:, i] = cp.gradient(y1, axis=0) / cp.gradient(X[:, i], axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mdy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             X, y = self._validate_data(X, y, multi_output=True, y_numeric=True,\n\u001b[0m\u001b[1;32m    194\u001b[0m                                        ensure_2d=True, dtype=\"numeric\")\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__array__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly."
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "import cupy as cp\n",
    "\n",
    "class GaussianRegressionWithDerivatives(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, kernel=None, alpha=0.0):\n",
    "        self.kernel = kernel\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Create kernel for Gaussian process regression\n",
    "        if self.kernel is None:\n",
    "            self.kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + \\\n",
    "                          WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-10, 1e2))\n",
    "        self.gpr = GaussianProcessRegressor(kernel=self.kernel, alpha=self.alpha, normalize_y=True)\n",
    "        \n",
    "        # Compute derivatives of y with respect to each feature\n",
    "        dy = cp.zeros((n_samples, n_features))\n",
    "        for i in range(n_features):\n",
    "            #X1 = cp.delete(X, i, axis=1)\n",
    "            X1 = cp.concatenate((X[:,:i], X[:,i+1:]), axis=1)\n",
    "            print('the shape of X1 after deleting one column is: ' + str(X1.shape))\n",
    "            #y1 = self.gpr.fit(X1, cp.gradient(y, axis=0)[:, i]).predict(X1, return_std=False)\n",
    "            y1 = self.gpr.fit(X1, cp.gradient(y, axis=0)).predict(X1, return_std=False)\n",
    "            #dy[:, i] = cp.gradient(y1, axis=0) / cp.gradient(X[:, i], axis=0)\n",
    "            dy[:, i] = cp.gradient(y1, axis=0) / cp.gradient(X[:, i], axis=0)\n",
    "            \n",
    "        # Fit the model with y and its derivatives\n",
    "        X_aug = cp.concatenate((X, dy), axis=1)\n",
    "        self.gpr.fit(X_aug, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Compute derivatives of X\n",
    "        dX = np.zeros((n_samples, n_features))\n",
    "        for i in range(n_features):\n",
    "            dX[:, i] = np.gradient(X[:, i], axis=0)\n",
    "        \n",
    "        # Predict y and its derivatives\n",
    "        X_aug = cp.concatenate((X, dX), axis=1)\n",
    "        y_pred = self.gpr.predict(X_aug, return_std=False)\n",
    "        dy_pred = self.gpr.predict(X_aug, return_std=False, jac=True)[1]\n",
    "        \n",
    "        return y_pred, dy_pred\n",
    "\n",
    "X1 = cp.array([[1.,2.,3.],[3.,4.,5.],[5.,6.,7.]])\n",
    "y = cp.array([2.,4.,6.])\n",
    "gr = GaussianRegressionWithDerivatives()\n",
    "gr.fit(X1,y)\n",
    "\n",
    "# use the same data set as training, y_pred should be similar to y\n",
    "y_pred, dy_pred = gr.predict(X1)\n",
    "\n",
    "print('y is: ' + str(y))\n",
    "print('y_pred is: ' + str(y_pred))\n",
    "print('dy_pred: \\n' + str(dy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 3]\n",
      " [4 6]\n",
      " [7 9]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# create a 3x3 matrix\n",
    "a = cp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# delete the second column (index 1) and modify the original array in place\n",
    "b = cp.concatenate((a[:,:1], a[:,2:]), axis=1)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# create a 2D array\n",
    "a = cp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# compute the gradient of the array\n",
    "grad_x, grad_y = cp.gradient(a)\n",
    "\n",
    "print(grad_x)\n",
    "print(grad_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
